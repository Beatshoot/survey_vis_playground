@article{Beck2016Visual,
  abstract  = {Bibiographic data such as collections of scientific articles and citation networks have been studied extensively in information visualization and visual analytics research. Powerful systems have been built to support various types of bibliographic analysis, but they require some training and cannot be used to disseminate the insights gained. In contrast, we focused on developing a more accessible visual analytics system, called SurVis, that is ready to disseminate a carefully surveyed literature collection. The authors of a survey may use our Web-based system to structure and analyze their literature database. Later, readers of the survey can obtain an overview, quickly retrieve specific publications, and reproduce or extend the original bibliographic analysis. Our system employs a set of selectors that enable users to filter and browse the literature collection as well as to control interactive visualizations. The versatile selector concept includes selectors for textual search, filtering by keywords and meta-information, selection and clustering of similar publications, and following citation links. Agreement to the selector is represented by word-sized sparkline visualizations seamlessly integrated into the user interface. Based on an analysis of the analytical reasoning process, we derived requirements for the system. We developed the system in a formative way involving other researchers writing literature surveys. A questionnaire study with 14 visual analytics experts confirms that SurVis meets the initially formulated requirements.},
  author    = {Beck, Fabian and Koch, Sebastian and Weiskopf, Daniel},
  doi       = {10.1109/TVCG.2015.2467757},
  journal   = {IEEE Transactions on Visualization and Computer Graphics},
  keywords  = {type:system, visual_analytics, sparklines, information_retrieval, clustering, literature_browser},
  number    = {01},
  publisher = {IEEE},
  series    = {TVCG},
  title     = {Visual Analysis and Dissemination of Scientific Literature Collections with {SurVis}},
  url       = {http://www.visus.uni-stuttgart.de/uploads/tx_vispublications/vast15-survis.pdf},
  volume    = {22},
  year      = {2016}
}

@inproceedings{white2006,
  abstract  = {the tools that botanists require for field-work must evolve and take on new forms. Of particular importance is the ability to identify existing and new species in the field. Mobile augmented reality systems can make it possible to access, view, and inspect a large database of virtual species examples side-by-side with physical specimens. In this paper, we present prototypes of a mobile augmented reality electronic field guide and techniques for displaying and inspecting computer vision-based visual search results in the form of virtual vouchers. Our work addresses head-movement controlled augmented reality for hands-free interaction and tangible augmented reality. We describe results from our design and investigation process and discuss observations and feedback from lab trials by botanists.},
  author    = {White, Sean and Feiner, Steven and Kopylec, Jason},
  booktitle = {3D User Interfaces},
  doi       = {10.1109/VR.2006.145},
  pages     = {119-126},
  publisher = {IEEE},
  title     = {Virtual Vouchers: Prototyping a Mobile Augmented Reality User Interface for Botanical Species Identification},
  url       = {https://ieeexplore.ieee.org/document/1647517},
  year      = {2006}
}

@inproceedings{reitberger2007,
  abstract  = {This paper discusses the prototypical implementation of an ambient display and the results of an empirical study in a retail store. It presents the context of shopping as an application area for Ambient Intelligence (AmI) technologies. The prototype consists of an ambient store map that enhances the awareness of customer activity. The results of our study indicate potentials and challenges for an improvement of the shopping experience with AmI technologies. Based on our findings we discuss challenges and future developments for applying AmI technologies to shopping environments.},
  author    = {Reitberger, Wolfgang and Obermair, Christoph and Ploderer, Bernd and Meschtscherjakov, Alexander and Tscheligi, Manfred},
  booktitle = {Ambient Intelligence},
  doi       = {10.1007/978-3-540-76652-0_19},
  url       = {https://link.springer.com/chapter/10.1007/978-3-540-76652-0_19},
  isbn      = {978-3-540-76652-0},
  pages     = {314--331},
  publisher = {Springer},
  title     = {Enhancing the Shopping Experience with Ambient Displays: A Field Study in a Retail Store},
  year      = {2007}
}

@inproceedings{white2009a,
  abstract  = {Menus play an important role in both information presentation and system control. We explore the design space of shake menus, which are intended for use in tangible augmented reality. Shake menus are radial menus displayed centered on a physical object and activated by shaking that object. One important aspect of their design space is the coordinate system used to present menu options. We conducted a within-subjects user study to compare the speed and efficacy of several alternative methods for presenting shake menus in augmented reality (world-referenced, display-referenced, and object-referenced), along with a baseline technique (a linear menu on a clipboard). Our findings suggest tradeoffs amongst speed, efficacy, and flexibility of interaction, and point towards the possible advantages of hybrid approaches that compose together transformations in different coordinate systems. We close by describing qualitative feedback from use and present several illustrative applications of the technique.}
  author    = {White, Sean and Feng, David and Feiner, Steven},
  booktitle = {8th IEEE International Symposium on Mixed and Augmented Reality},
  doi       = {10.1109/ISMAR.2009.5336500},
  url       = {https://ieeexplore.ieee.org/document/5336500},
  pages     = {39-48},
  title     = {Interaction and Presentation Techniques for Shake Menus in Tangible Augmented Reality},
  year      = {2009}
}

@inproceedings{white2009b,
  author = {White, Sean and Feiner, Steven},
  title = {SiteLens: Situated Visualization Techniques for Urban Site Visits},
  year = {2009},
  isbn = {9781605582467},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/1518701.1518871},
  doi = {10.1145/1518701.1518871},
  abstract = {Urban designers and urban planners often conduct site visits prior to a design activity to search for patterns or better understand existing conditions. We introduce SiteLens, an experimental system and set of techniques for supporting site visits by visualizing relevant virtual data directly in the context of the physical site, which we call situated visualization. We address alternative visualization representations and techniques for data collection, curation, discovery, comparison, manipulation, and provenance. A real use scenario is presented and two iterations of evaluation with faculty and students from the Columbia University Graduate School of Architecture, Planning and Preservation provide directions and insight for further investigation.},
  booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
  pages = {1117–1120},
  numpages = {4},
  location = {Boston, MA, USA},
  series = {CHI '09}
}

@inbook{kalkofen2011,
  abstract   = {
  Visualizations in real world environments benefit from the visual interaction between real and virtual imagery. However, compared to traditional visualizations, a number of problems have to be solved in order to achieve effective visualizations within Augmented Reality (AR). This chapter provides an overview of techniques to handle the main obstacles in AR visualizations. It discusses spatial integration of virtual objects within real world environments, techniques to rearrange objects within mixed environments, and visualizations which adapt to its environmental context.},
  author     = {Kalkofen, Denis and Sandor, Christian and White, Sean and Schmalstieg, Dieter},
  booktitle  = {Handbook of Augmented Reality},
  doi        = {10.1007/978-1-4614-0064-6_3},
  pages      = {65--98},
  publisher  = {Springer},
  title      = {Visualization Techniques for Augmented Reality},
  url        = {https://arbook.icg.tugraz.at/schmalstieg/Schmalstieg_217.pdf},
  year       = {2011},
}

@inproceedings{walsh2011,
  author = {Walsh, James A. and Thomas, Bruce H.},
  title = {Visualising Environmental Corrosion in Outdoor Augmented Reality},
  year = {2011},
  isbn = {9781920682972},
  publisher = {Australian Computer Society, Inc.},
  address = {AUS},
  abstract = {This paper provides a description of outdoor visualisation of environmental corrosion data. This system was developed to aid in the visual understanding of data from wireless sensors used to monitor large structures. Due to the laborious manual inspections required for large structures (such as bridges), wireless environmental sensors have been designed to automate this process. Our system visualizes this information in its real-world context using the Tinmith mobile outdoor augmented reality system. We provide an overview of the visualizations, outlining a user study that was conducted to determine the effectiveness of the visualizations in providing the user with context-sensitive information, along with the preliminary results of this study. The paper concludes with an overview of future work on the system and final thoughts.},
  booktitle = {Proceedings of the Twelfth Australasian User Interface Conference - Volume 117},
  pages = {39–46},
  numpages = {8},
  keywords = {augmented reality, corrosion, environment visualisation, wireless sensor, visualisation},
  location = {Perth, Australia},
  series = {AUIC '11},
  doi = {10.5555/2460616.2460621},
  url = {https://crpit.scem.westernsydney.edu.au/confpapers/CRPITV117Walsh.pdf}
}

  

@article{vandemoere2012,
author = {Vande Moere, Andrew and Hill, Dan},
title = {Designing for the Situated and Public Visualization of Urban Data},
journal = {Journal of Urban Technology},
volume = {19},
number = {2},
pages = {25-46},
year  = {2012},
publisher = {Routledge},
doi = {10.1080/10630732.2012.698065},
URL = {https://doi.org/10.1080/10630732.2012.698065},
abstract = { This paper investigates the concept of urban visualization, the visual representation of an urban environment through its intrinsic or related data, where its display is also situated within that physical environment. It describes how the principles behind public and urban displays can be combined with those of social visualization and persuasive computing in order to create discursive as well as pictorial representations that provide a better and potentially actionable understanding of urban issues to its inhabitants. We introduce the role of several related research fields, and analyze a set of representative case studies, taken from current best practice, academic research studies, and an experimental design studio course to highlight the typical issues involved in conceptualizing and implementing an urban visualization. Lastly, the paper proposes a set of design constraints that typically characterize an urban visualization, in order to guide the future design and evaluation of useful applications within the field. }
}



@article{jose2014,
author = {Jos\'{e}, Rui and Otero, Nuno and Cardoso, Jorge C. S.},
title = {Dimensions of Situatedness for Digital Public Displays},
year = {2015},
issue_date = {January 2014},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2014},
issn = {1687-5893},
url = {https://doi.org/10.1155/2014/474652},
doi = {10.1155/2014/474652},
abstract = {Public displays are often strongly situated signs deeply embedded in their physical, social, and cultural setting. Understanding how the display is coupled with on-going situations, its level of situatedness, provides a key element for the interpretation of the displays themselves but is also an element for the interpretation of place, its situated practices, and its social context. Most digital displays, however, do not achieve the same sense of situatedness that seems so natural in their nondigital counterparts. This paper investigates people's perception of situatedness when considering the connection between public displays and their context. We have collected over 300 photos of displays and conducted a set of analysis tasks involving focus groups and structured interviews with 15 participants. The contribution is a consolidated list of situatedness dimensions that should provide a valuable resource for reasoning about situatedness in digital displays and informing the design and development of display systems.},
journal = {Adv. in Hum.-Comp. Int.},
month = jan,
articleno = {16},
numpages = {1}
}

  
@inproceedings{claes2015,
author = {Claes, Sandy and Vande Moere, Andrew},
title = {The Role of Tangible Interaction in Exploring Information on Public Visualization Displays},
year = {2015},
isbn = {9781450336086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2757710.2757733},
doi = {10.1145/2757710.2757733},
abstract = {A rising number of public displays are becoming equipped with tangible interfaces. Especially in the context of the visualization of data in the public realm, offering tangible interaction modalities might actively attract and engage passer-bys, and lead to increased information discovery.. We therefore present a novel public visualization installation that deploys different forms of tangible interaction in combination with a public display in order to communicate civic data to a lay audience. During a comparative, deployment-based study in an urban context, we compared three distinct tangible interaction modalities in terms of the types of engagement and insight generation they facilitated. We report on our findings and discuss a number of design recommendations for tangible interaction on public information displays.},
booktitle = {Proceedings of the 4th International Symposium on Pervasive Displays},
pages = {201–207},
numpages = {7},
keywords = {Tangible interaction, physical visualization, public display, public visualization, urban informatics},
location = {Saarbruecken, Germany},
series = {PerDis '15}
}

@article{valkanova2015,
  title = {Public visualization displays of citizen data: Design, impact and implications},
journal = {International Journal of Human-Computer Studies},
volume = {81},
pages = {4-16},
year = {2015},
note = {Transdisciplinary Approaches to Urban Computing},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2015.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S1071581915000282},
author = {Valkanova, Nina and Jorda, Sergi and Vande Moere, Andrew},
keywords = {Public display, Urban screen, Urban visualization, In-the-wild study, Awareness, Reflection, Discussion, Evaluation, Sustainability, Civic engagement, Persuasive technology},
abstract = {In this paper we propose citizen-driven, public data visualization as a tool to support social and civic purposes in public spaces. We argue for the potential of this approach, motivating it with recent trends and developments in the areas of information visualization, urban computing, and urban screens, and we layout a transdisciplinary research approach and methodology. Through three studies approaching our research goal from design, empirical, and reflective perspectives, we show how visualization interfaces, situated in public spaces can improve perception, and lead to sustained behavior change; can increase social awareness and discourse; and can influence meaningful participation and a range of social interactions related to locally relevant topics. We conclude by discussing implications for the design, use and evaluation of citizen-driven public visualization as a tool increase public awareness, participation and discourse.}
}

@inproceedings{thompson2016,
abstract = {In this paper, we present the results of a multi-year participatory design process exploring the space of educational AR experiences for STEM education targeted at students of various ages and abilities. Our participants included teachers, students (ages five to fourteen), educational technology experts, game designers, and HCI researchers. The work was informed by state educational curriculum guidelines. The activities included developing a set of design dimensions which guided our ideation process, iteratively designing, building, and evaluating six prototypes with our stakeholders, and collecting our observations regarding the use of AR STEM applications by target students.},
  author       = {Thompson, Ben and Levy, Laura and Lambeth, Amelia and Byrd, David and Alcaidinho, Joelle and Radu, Iulian and Gandy, Maribeth},
  booktitle    = {2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)},
  organization = {IEEE},
  pages        = {53--58},
  title        = {Participatory design of STEM education AR experiences for heterogeneous student groups: exploring dimensions of tangibility, simulation, and interaction},
  year         = {2016},
  doi          = {10.1109/ISMAR-Adjunct.2016.0038},
  url = {https://ieeexplore.ieee.org/document/7836459}

}

@inproceedings{zollman2016,
  author={Zollmann, Stefanie and Poglitsch, Christian and Ventura, Jonathan},
  booktitle={2016 International Conference on Image and Vision Computing New Zealand (IVCNZ)}, 
  title={VISGIS: Dynamic situated visualization for geographic information systems}, 
  year={2016},
  volume={},
  number={},
  pages={1-6},
  url             = {https://ieeexplore.ieee.org/document/7804440},
  doi             = {10.1109/IVCNZ.2016.7804440},
  abstract        = {Situated Visualization techniques are visualization techniques that provide a presentation of information within its spatial context. Situated Visualization techniques have several advantages compared to traditional visualization techniques with the biggest advantage being providing the spatial relationship between data and the actual environment. However, Situated Visualization techniques are also subject to several challenges. In particular, Situated Visualization of data from geographic information systems (GIS) is exposed to a set of problems, such as limited visibility, legibility, information clutter and the limited understanding of spatial relationships. In this paper, we address the challenges of visibility, information clutter and understanding of spatial relationships with a set of dynamic Situated Visualization techniques that address the special needs of Situated Visualization of GIS data in particular for “street-view”-like perspectives as used for many navigation applications. The proposed techniques include dynamic annotation placement, dynamic label alignment and occlusion culling. We applied those techniques for two types of Situated Visualizations: Augmented Reality visualization and Indirect Augmented Reality using 360 Degree footage.}
}

@article{elsayed2016,
title = {Situated Analytics: Demonstrating immersive analytical tools with Augmented Reality},
journal = {Journal of Visual Languages & Computing},
volume = {36},
pages = {13-23},
year = {2016},
issn = {1045-926X},
doi = {https://doi.org/10.1016/j.jvlc.2016.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S1045926X16300404},
author = {Neven A.M. ElSayed and Bruce H. Thomas and Kim Marriott and Julia Piantadosi and Ross T. Smith},
keywords = {Augmented Reality, Information visualization, Situated Analytics, Visual Analytics, Visualization, Shopping application},
abstract = {This paper introduces the use of Augmented Reality as an immersive analytical tool in the physical world. We present Situated Analytics, a novel combination of real-time interaction and visualization techniques that allows exploration and analysis of information about objects in the user's physical environment. Situated Analytics presents both situated and abstract summary and contextual information to a user. We conducted a user study to evaluate its use in three shopping analytics tasks, comparing the use of a Situated Analytics prototype with manual analysis. The results showed that users preferred the Situated Analytics prototype over the manual method, and that tasks were performed more quickly and accurately using the prototype.}
}

@inproceedings{engelke2016,
author = {Engelke, Ulrich and Hutson, Holly and Nguyen, Huyen and de Souza, Paulo},
title = {MelissAR: Towards Augmented Visual Analytics of Honey Bee Behaviour},
year = {2016},
isbn = {9781450340823},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851581.2892367},
doi = {10.1145/2851581.2892367},
abstract = {We present the design and current prototype implementation of MelissAR, an augmented reality system for visual analytics of honey bee behaviour in the field. The system is intended to support bee keepers and other relevant users to monitor honey bee populations and to make effective decisions based on their status. The implementation of MelissAR is based on informed design choices with regard to usability in the field, effective communication of relevant information, and robustness to varying outdoor conditions.},
booktitle = {Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems},
pages = {2057–2063},
numpages = {7},
keywords = {augmented reality, user interface design, information visualisation, decision support system},
location = {San Jose, California, USA},
series = {CHI EA '16}
}

@inproceedings{singhal2017,
author = {Singhal, Samarth and Odom, William and Bartram, Lyn and Neustaedter, Carman},
title = {Time-Turner: Data Engagement Through Everyday Objects in the Home},
year = {2017},
isbn = {9781450349918},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3064857.3079122},
doi = {10.1145/3064857.3079122},
abstract = {Families enjoy capturing digital media about their life and replaying moments, yet it is not always easy to do so. To explore this design space, we created a physical, ambient, and situated visualization prototype called Time-Turner specifically designed for a home setting that records video of family activities and allows families to review their past activities. We report our design requirements, design rationale and the implications of our work for future design researchers.},
booktitle = {Proceedings of the 2017 ACM Conference Companion Publication on Designing Interactive Systems},
pages = {72–78},
numpages = {7},
keywords = {ambient systems, tangible visualization, situated visualization, data physicalization},
location = {Edinburgh, United Kingdom},
series = {DIS '17 Companion}
}

@inproceedings{li2017,
author = {Li, Nico and Sharlin, Ehud and Sousa, Mario Costa},
title = {Duopography: Using Back-of-Device Multi-Touch Input to Manipulate Spatial Data on Mobile Tangible Interactive Topography},
year = {2017},
isbn = {9781450354103},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132787.3139197},
doi = {10.1145/3132787.3139197},
abstract = {In this short paper we present the design of Duopography1, a dual-surface mobile tangible interface for spatial representation and manipulation of topography. The 3D physical topographic front of Duopography acts as a tangible interface, enabling sketching directly on the 3D terrain, as well as visual augmentation of the topography. At the same time, Duopography's flat back-of-device supports gestures that are hard to perform on the irregular front, allowing common interaction techniques such as panning and pinching. We contribute a prototype and the results of a preliminary evaluation of a dual-surface topography interface combining 3D printed front and a flat back-of-device.},
booktitle = {SIGGRAPH Asia 2017 Mobile Graphics &amp; Interactive Applications},
articleno = {20},
numpages = {6},
keywords = {tangible user interfaces, mobile interaction, back-of-device input, dual-surface interaction, augmented reality, topography},
location = {Bangkok, Thailand},
series = {SA '17}
}

@article{Willett:2017,
  author     = {Willett, Wesley and Jansen, Yvonne and Dragicevic, Pierre},
  doi        = {10.1109/TVCG.2016.2598608},
  issn       = {1077-2626},
  journal    = {IEEE TVCG},
  month      = jan,
  number     = {1},
  numpages   = {10},
  pages      = {461--470},
  title      = {{Embedded Data Representations}},
  url        = {https://doi.org/10.1109/TVCG.2016.2598608},
  volume     = {23},
  year       = {2017},
  abstract   = {We introduce embedded data representations, the use of visual and physical representations of data that are deeply integrated with the physical spaces, objects, and entities to which the data refers. Technologies like lightweight wireless displays, mixed reality hardware, and autonomous vehicles are making it increasingly easier to display data in-context. While researchers and artists have already begun to create embedded data representations, the benefits, trade-offs, and even the language necessary to describe and compare these approaches remain unexplored. In this paper, we formalize the notion of physical data referents - the real-world entities and spaces to which data corresponds - and examine the relationship between referents and the visual and physical representations of their data. We differentiate situated representations, which display data in proximity to data referents, and embedded representations, which display data so that it spatially coincides with data referents. Drawing on examples from visualization, ubiquitous computing, and art, we explore the role of spatial indirection, scale, and interaction for embedded representations. We also examine the tradeoffs between non-situated, situated, and embedded data displays, including both visualizations and physicalizations. Based on our observations, we identify a variety of design challenges for embedded data representation, and suggest opportunities for future research and applications.}
}

@inproceedings{hull2017,
author = {Hull, Carmen and Willett, Wesley},
title = {Building with Data: Architectural Models as Inspiration for Data Physicalization},
year = {2017},
isbn = {9781450346559},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025453.3025850},
doi = {10.1145/3025453.3025850},
abstract = {In this paper we analyze the role of physical scale models in the architectural design process and apply insights from architecture for the creation and use of data physicalizations. Based on a survey of the architecture literature on model making and ten interviews with practicing architects, we describe the role of physical models as a tool for exploration and communication. From these observations, we identify trends in the use of physical models in architecture, which have the potential to inform the design of data physicalizations. We identify four functions of architectural modeling that can be directly adapted for use in the process of building rich data models. Finally, we discuss how the visualization community can apply observations from architecture to the design of new data physicalizations.},
booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
pages = {1217–1264},
numpages = {48},
keywords = {data physicalization, embodied interaction, architectural models, data visualization, design process},
location = {Denver, Colorado, USA},
series = {CHI '17}
}

@inproceedings{bueschel2018,
author = {B\"{u}schel, Wolfgang and Mitschick, Annett and Dachselt, Raimund},
title = {Here and Now: Reality-Based Information Retrieval: Perspective Paper},
year = {2018},
isbn = {9781450349253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3176349.3176384},
doi = {10.1145/3176349.3176384},
abstract = {Today, the widespread use of mobile devices allows users to search information "on the go»», whenever and wherever they want, no longer confining Information Retrieval to classic desktop interfaces. We believe that technical advances in Augmented Reality will allow Information Retrieval to go even further, making use of both the users» surroundings and their abilities to interact with the physical world. In this paper, we present the fundamental concept of Reality-Based Information Retrieval, which combines the classic Information Retrieval process with Augmented Reality technologies to provide context-dependent search cues and situated visualizations of the query and the results. With information needs often stemming from real-world experiences, this novel combination has the potential to better support both Just-in-time Information Retrieval and serendipity. Based on extensive literature research, we propose a conceptual framework for Reality-Based Information Retrieval. We illustrate and discuss this framework and present two prototypical implementations, which we tested in small user studies. They demonstrate the feasibility of our concepts and inspired our discussion of notable challenges for further research in this novel and promising area.},
booktitle = {Proceedings of the 2018 Conference on Human Information Interaction &amp; Retrieval},
pages = {171–180},
numpages = {10},
keywords = {augmented reality, in situ visual analytics, spatial user interface, immersive visualization, reality-based information retrieval},
location = {New Brunswick, NJ, USA},
series = {CHIIR '18}
}

@article{thomas2018,
  TITLE = {{Situated Analytics}},
  AUTHOR = {Thomas, Bruce and Welch, Gregory and Dragicevic, Pierre and Elmqvist, Niklas and Irani, Pourang and Jansen, Yvonne and Schmalstieg, Dieter and Tabard, Aur{\'e}lien and ElSayed, Neven and Smith, Ross and Willett, Wesley},
  URL = {https://hal.inria.fr/hal-01947243},
  BOOKTITLE = {{Immersive Analytics}},
  PUBLISHER = {{Springer}},
  SERIES = {Lecture Notes in Computer Science},
  VOLUME = {11190},
  PAGES = {185-220},
  YEAR = {2018},
  MONTH = Oct,
  DOI = {10.1007/978-3-030-01388-2\_7},
  KEYWORDS = {Data visualisation ; Situated analytics ; Immersive analytics ; Immersion ; Human-computer interaction ; Augmented reality},
  PDF = {https://hal.inria.fr/hal-01947243/file/SituatedAnalytics.pdf},
  HAL_ID = {hal-01947243},
  HAL_VERSION = {v1},
}


@inproceedings{bressa2019,
author = {Bressa, Nathalie and Wannamaker, Kendra and Korsgaard, Henrik and Willett, Wesley and Vermeulen, Jo},
title = {Sketching and Ideation Activities for Situated Visualization Design},
year = {2019},
isbn = {9781450358507},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3322276.3322326},
doi = {10.1145/3322276.3322326},
abstract = {We report on findings from seven design workshops that used ideation and sketching activities to prototype new situated visualizations - representations of data that are displayed in proximity to the physical referents (such as people, objects, and locations) to which the data is related. Designing situated visualizations requires a fine-grained understanding of the context in which the visualizations are placed, as well as an exploration of different options for placement and form factors, which existing methods for visualization design do not account for. Focusing on small displays as a target platform, we reflect on our experiences of using a diverse range of sketching activities, materials, and prompts. Based on these observations, we identify challenges and opportunities for sketching and ideating situated visualizations. We also outline the space of design activities for situated visualization and highlight promising methods for both designers and researchers.},
booktitle = {Proceedings of the 2019 on Designing Interactive Systems Conference},
pages = {173–185},
numpages = {13},
keywords = {ideation, design workshops, situated visualization, information visualization, sketching, small displays},
location = {San Diego, CA, USA},
series = {DIS '19}
}

@inproceedings{demacedomorais2019,
  author={de Macêdo Morais, Luiz Augusto and Andrade, Nazareno and Costa de Sousa, Dandara Maria and Ponciano, Lesandro},
  booktitle={2019 IEEE Pacific Visualization Symposium (PacificVis)}, 
  title={Defamiliarization, Representation Granularity, and User Experience: A Qualitative Study with Two Situated Visualizations}, 
  year={2019},
  volume={},
  number={},
  pages={92-101},
  doi={10.1109/PacificVis.2019.00019},
  url             = {https://ieeexplore.ieee.org/document/8781563},
  abstract = {This work explores the user experience with two situated visualizations that lie on different points of design space. The first visualization - the Activity Clock - displays the aggregate presence of laboratory members into a wall clock. The second - Personal Activities - represents the same persons individually, in a conventional poster media. We interviewed 17 participants and leverage a theoretical lens of Continuous Engagement and Sense-Making to study how design decisions impact the user experience with respect to (1) which design factors attract users, (2) how design features affect users' understanding of the visualization, and (3) what kind of reflections are evoked by design. We discuss how the defamiliarizing effect of the Activity Clock plays a dual role in attracting users while also hindering their understanding of the data. We also consider the evidence that fine representation granularity in the Personal Activities evokes deeper reflections.}
}

@inproceedings{coenen2019,
author = {Coenen, Jorgos and Houben, Maarten and Vande Moere, Andrew},
title = {Citizen Dialogue Kit: Public Polling and Data Visualization Displays for Bottom-Up Citizen Participation},
year = {2019},
isbn = {9781450362702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3301019.3325160},
doi = {10.1145/3301019.3325160},
abstract = {In this demo we introduce our ongoing research on how to leverage the situated visualization of open and citizen science data within public space to inform and engage citizens. We developed an open-source toolkit, coined "Citizen Dialogue Kit" that is able to convey data visualizations on a set of interactive, wirelessly networked displays that can be freely positioned in urban space. The toolkit consists of a participative methodology to guide stakeholders with the choice of data and the design of its visualization, a set of off-the-shelf hardware components, and custom-made open source software that controls the whole system. We summarize the design of the toolkit and its initial deployment and conclude by discussing implications for urban visualization and future work.},
booktitle = {Companion Publication of the 2019 on Designing Interactive Systems Conference 2019 Companion},
pages = {9–12},
numpages = {4},
keywords = {situated visualization, citizen science, participation, public displays, smart cities, civic technology, open data, urban visualization, public visualization},
location = {San Diego, CA, USA},
series = {DIS '19 Companion}
}

  

@inproceedings{marques2019,
  author={Marques, Bernardo and Santos, Beatriz Sousa and Araújo, Tiago and Martins, Nuno Cid and Alves, João Bernardo and Dias, Paulo},
  booktitle = {2019 23rd International Conference Information Visualisation (IV)}, 
  pages     = {13-18},
  publisher = {IEEE},
  title     = {{Situated Visualization in The Decision Process Through Augmented Reality}},
  year      = {2019},
  doi       = {10.1109/IV.2019.00012},
  url       = {https://ieeexplore.ieee.org/document/8811981},
  abstract  = {The decision-making process and the development of decision support systems (DSS) have been enhanced by a variety of methods originated from information science, cognitive psychology and artificial intelligence over the past years. Situated visualization (SV) is a method to present data representations in context. Its main characteristic is to display data representations near the data referent. As augmented reality (AR) is becoming more mature, affordable and widespread, using it as a tool for SV becomes feasible in several situations. In addition, it may provide a positive contribution to more effective and efficient decision-making, as the users have contextual, relevant and appropriate information to endorse their choices. As new challenges and opportunities arise, it is important to understand the relevance of intertwining these fields. Based on a literature analysis, this paper addresses and discusses current areas of application, benefits, challenges and opportunities of using SV through AR to visualize data in context and to support a decision-making process and its importance in future DSS.}
}

@inproceedings{caggianese2019,
  author     = {Caggianese, Giuseppe and Colonnese, Valerio and Gallo, Luigi},
  booktitle  = {2019 15th International Conference on Signal-Image Technology Internet-Based Systems (SITIS)}, 
  doi        = {10.1109/SITIS.2019.00069},
  pages      = {390-395},
  title      = {{Situated Visualization in Augmented Reality: Exploring Information Seeking Strategies}},
  year       = {2019},
  url        = {https://ieeexplore.ieee.org/document/9067881},
  abstract   = {In recent years augmented reality applications have been increasingly demonstrating the requirement for an interaction with information related to and directly shown in the surrounding environment. Situated information is visualized in its semantic and spatial context, building up an environment enhanced by an information level that dynamically adapts to the production of the information and to the actions of the user. The exploration and manipulation of this type of data through see-through augmented reality devices still represents a challenging task. The development of specific interaction strategies capable to mitigating the current limitations of augmented reality devices is essential. In this context, our contribution has been to design possible solutions to address some of these challenges allowing a dynamic interaction with situated information. Following the visual "information-seeking mantra" proposed by Shneiderman and introducing some "superpowers" for the users, in this work we present different strategies aimed at obtaining an overview and filtering, and acquiring details of a collection of situated data.}
}

@article{francia2020,
  author     = {Matteo Francia and Matteo Golfarelli and Stefano Rizzi},
  doi        = {https://doi.org/10.1016/j.is.2020.101520},
  issn       = {0306-4379},
  journal    = {Information Systems},
  keywords   = {Augmented reality, OLAP, Query recommendation},
  pages      = {101520},
  title      = {{A-BI+: A Framework for Augmented Business Intelligence}},
  url        = {http://www.sciencedirect.com/science/article/pii/S0306437920300314},
  volume     = {92},
  year       = {2020},
  abstract   = {Augmented reality allows users to superimpose digital information (typically, of operational type) upon real-world objects. The synergy of analytical frameworks and augmented reality opens the door to a new wave of situated analytics, in which users within a physical environment are provided with immersive analyses of local contextual data. In this paper, we propose an approach named A-BI+ (Augmented Business Intelligence) that, based on the sensed augmented context (provided by wearable and smart devices), proposes a set of relevant analytical queries to the user. This is done by relying on a mapping between the objects that can be recognized by the devices and the elements of the enterprise multidimensional cubes, and also by taking into account the queries preferred by users during previous interactions that occurred in similar contexts. A set of experimental tests evaluates the proposed approach in terms of efficiency, effectiveness, and user satisfaction.}
}

@article{whitlock2020TVCG,
author={Whitlock, Matt and Wu, Keke and Szafir, Danielle Albers},  
journal={IEEE Transactions on Visualization and Computer Graphics},   
title={Designing for Mobile and Immersive Visual Analytics in the Field},  
year={2020},  
volume={26},  
number={1},  
pages={503-513},  
abstract={Data collection and analysis in the field is critical for operations in domains such as environmental science and public safety. However, field workers currently face dataand platform-oriented issues in efficient data collection and analysis in the field, such as limited connectivity, screen space, and attentional resources. In this paper, we explore how visual analytics tools might transform field practices by more deeply integrating data into these operations. We use a design probe coupling mobile, cloud, and immersive analytics components to guide interviews with ten experts from five domains to explore how visual analytics could support data collection and analysis needs in the field. The results identify shortcomings of current approaches and target scenarios and design considerations for future field analysis systems. We embody these findings in FieldView, an extensible, open-source prototype designed to support critical use cases for situated field analysis. Our findings suggest the potential for integrating mobile and immersive technologies to enhance data's utility for various field operations and new directions for visual analytics tools to transform fieldwork.},  keywords={},  
doi={10.1109/TVCG.2019.2934282},  
ISSN={1941-0506},  
month={Jan}
}


@inproceedings{guarese2020,
  author        = {Guarese, Renan and Nilsson, Emil and Andreasson, Pererik and Maciel, Anderson},
  booktitle     = {Proc. XChange Reality! 2020},
  date-modified = {2021-06-26 00:04:22 +0200},
  title         = {A Proposal for Augmented Situated Visualization Towards EMC Testing},
  year          = {2020}
}

@article{quach2020,
  author     = {Quang Quach and Bernhard Jenny},
  doi        = {10.1080/15230406.2020.1771771},
  journal    = {Cartogr. Geogr. Inf. Sci},
  number     = {6},
  pages      = {471-480},
  publisher  = {Taylor & Francis},
  title      = {{Immersive Visualization with Bar Graphics}},
  volume     = {47},
  year       = {2020},
  bdsk-url-1 = {https://doi.org/10.1080/15230406.2020.1771771}
}

@inproceedings{chen2020,
  author     = {Chen, Zhutian and Tong, Wai and Wang, Qianwen and Bach, Benjamin and Qu, Huamin},
  booktitle  = {Proc. CHI '20},
  doi        = {10.1145/3313831.3376436},
  isbn       = {9781450367080},
  pages      = {1--12},
  publisher  = {ACM},
  title      = {{Augmenting Static Visualizations with PapARVis Designer}},
  url        = {https://doi.org/10.1145/3313831.3376436},
  year       = {2020},
  bdsk-url-1 = {https://doi.org/10.1145/3313831.3376436}
}

@article{lobo2020,
  author    = {Lobo, Mar{\'\i}a-Jes{\'u}s and Christophe, Sidonie},
  journal   = {{ISPRS Annals}},
  pages     = {163--170},
  publisher = {Copernicus GmbH},
  title     = {{Opportunities and Challenges for Augmented Reality Situated Geographical Visualization}},
  volume    = {4},
  year      = {2020}
}

@inproceedings{prouzeau2020,
  author     = {Prouzeau, Arnaud and Wang, Yuchen and Ens, Barrett and Willett, Wesley and Dwyer, Tim},
  booktitle  = {{Proc. AVI '20}},
  doi        = {10.1145/3399715.3399743},
  publisher  = {ACM},
  title      = {{Corsican Twin: Authoring In Situ Augmented Reality Visualisations in Virtual Reality}},
  url        = {https://hal.archives-ouvertes.fr/hal-02614521},
  year       = {2020},
  bdsk-url-1 = {https://hal.archives-ouvertes.fr/hal-02614521},
  bdsk-url-2 = {https://doi.org/10.1145/3399715.3399743}
}

@inproceedings{merino2020,
  author     = {Merino, Leonel and Sotomayor-G\'{o}mez, Boris and Yu, Xingyao and Salgado, Ronie and Bergel, Alexandre and Sedlmair, Michael and Weiskopf, Daniel},
  booktitle  = {CHI EA '20},
  doi        = {10.1145/3334480.3383017},
  isbn       = {9781450368193},
  pages      = {1--7},
  publisher  = {ACM},
  title      = {{Toward Agile Situated Visualization: An Exploratory User Study}},
  url        = {https://doi.org/10.1145/3334480.3383017},
  year       = {2020},
  bdsk-url-1 = {https://doi.org/10.1145/3334480.3383017}
}

@article{perovich2020,
  author     = {L. J. Perovich and S. Wylie and R. Bongiovanni},
  doi        = {10.1109/TVCG.2020.3030472},
  journal    = {IEEE TVCG},
  number     = {2},
  pages      = {913-923},
  title      = {{Chemicals in the Creek: Designing a Situated Data Physicalization of Open Government Data with the Community}},
  volume     = {27},
  year       = {2021},
  bdsk-url-1 = {https://doi.org/10.1109/TVCG.2020.3030472}
}

@article{weiss2020,
  author     = {M. {Wei{\ss}} and K. {Angerbauer} and A. {Voit} and M. {Schwarzl} and M. {Sedlmair} and S. {Mayer}},
  doi        = {10.1109/TVCG.2020.3030400},
  journal    = {IEEE TVCG},
  number     = {2},
  pages      = {1204-1213},
  title      = {{Revisited: Comparison of Empirical Methods to Evaluate Visualizations Supporting Crafting and Assembly Purposes}},
  volume     = {27},
  year       = {2021},
  bdsk-url-1 = {https://doi.org/10.1109/TVCG.2020.3030400}
}

@article{morais2020,
  author     = {L. Morais and Y. Jansen and N. Andrade and P. Dragicevic},
  doi        = {10.1109/TVCG.2020.3023013},
  issn       = {1941-0506},
  journal    = {IEEE TVCG},
  pages      = {1-1},
  publisher  = {IEEE},
  title      = {{Showing Data about People: A Design Space of Anthropographics}},
  year       = {2020},
  bdsk-url-1 = {https://doi.org/10.1109/TVCG.2020.3023013}
}

@inproceedings{guarese2020augmented,
  address    = {New York, NY, USA},
  articleno  = {48},
  author     = {Guarese, Renan and Becker, Jo\~{a}o and Fensterseifer, Henrique and Walter, Marcelo and Freitas, Carla and Nedel, Luciana and Maciel, Anderson},
  booktitle  = {Proceedings of the International Conference on Advanced Visual Interfaces},
  doi        = {10.1145/3399715.3399838},
  isbn       = {9781450375351},
  keywords   = {Situated Visualization, Augmented Reality, Human Computer Interaction, Data Visualization},
  location   = {Salerno, Italy},
  numpages   = {5},
  publisher  = {ACM},
  series     = {AVI '20},
  title      = {Augmented Situated Visualization for Spatial and Context-Aware Decision-Making},
  url        = {https://doi.org/10.1145/3399715.3399838},
  year       = {2020},
  bdsk-url-1 = {https://doi.org/10.1145/3399715.3399838}
}

@inproceedings{Alallah2020,
  articleno  = {15},
  author     = {Alallah, Fouad and Sakamoto, Yumiko and Irani, Pourang},
  booktitle  = {Proc. SUI '20},
  doi        = {10.1145/3385959.3418458},
  numpages   = {11},
  publisher  = {ACM},
  title      = {{Exploring the Need and Design for Situated Video Analytics}},
  url        = {https://doi.org/10.1145/3385959.3418458},
  year       = {2020},
  bdsk-url-1 = {https://doi.org/10.1145/3385959.3418458}
}

@inproceedings{whitlock2020ISMAR,
  author     = {Whitlock, Matt and Szafir, Danielle Albers and Gruchalla, Kenny},
  booktitle  = {2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
  doi        = {10.1109/ISMAR50242.2020.00101},
  pages      = {704-712},
  title      = {HydrogenAR: Interactive Data-Driven Presentation of Dispenser Reliability},
  year       = {2020},
  bdsk-url-1 = {https://doi.org/10.1109/ISMAR50242.2020.00101}
}

@inproceedings{ens2021grandchallenges,
  address    = {New York, NY, USA},
  articleno  = {459},
  author     = {Ens, Barrett and Bach, Benjamin and Cordeil, Maxime and Engelke, Ulrich and Serrano, Marcos and Willett, Wesley and Prouzeau, Arnaud and Anthes, Christoph and B\"{u}schel, Wolfgang and Dunne, Cody and Dwyer, Tim and Grubert, Jens and Haga, Jason H. and Kirshenbaum, Nurit and Kobayashi, Dylan and Lin, Tica and Olaosebikan, Monsurat and Pointecker, Fabian and Saffo, David and Saquib, Nazmus and Schmalstieg, Dieter and Szafir, Danielle Albers and Whitlock, Matt and Yang, Yalong},
  booktitle  = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
  isbn       = {9781450380966},
  numpages   = {17},
  publisher  = {Association for Computing Machinery},
  title      = {Grand Challenges in Immersive Analytics},
  url        = {https://doi.org/10.1145/3411764.3446866},
  year       = {2021},
  bdsk-url-1 = {https://doi.org/10.1145/3411764.3446866}
}
