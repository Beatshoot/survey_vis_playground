@inproceedings{white2006,
  abstract  = {the tools that botanists require for field-work must evolve and take on new forms. Of particular importance is the ability to identify existing and new species in the field. Mobile augmented reality systems can make it possible to access, view, and inspect a large database of virtual species examples side-by-side with physical specimens. In this paper, we present prototypes of a mobile augmented reality electronic field guide and techniques for displaying and inspecting computer vision-based visual search results in the form of virtual vouchers. Our work addresses head-movement controlled augmented reality for hands-free interaction and tangible augmented reality. We describe results from our design and investigation process and discuss observations and feedback from lab trials by botanists.},
  author    = {White, Sean and Feiner, Steven and Kopylec, Jason},
  booktitle = {3D User Interfaces},
  doi       = {10.1109/VR.2006.145},
  pages     = {119-126},
  publisher = {IEEE},
  title     = {Virtual Vouchers: Prototyping a Mobile Augmented Reality User Interface for Botanical Species Identification},
  series    = {3DUI},
  url       = {https://ieeexplore.ieee.org/document/1647517},
  year      = {2006},
  keywords  = {definition:own_definition, technology:AR, data:science, methodology:lab_study, methodology:field_study, visualization:N/A}
}

@inproceedings{reitberger2007,
  abstract  = {This paper discusses the prototypical implementation of an ambient display and the results of an empirical study in a retail store. It presents the context of shopping as an application area for Ambient Intelligence (AmI) technologies. The prototype consists of an ambient store map that enhances the awareness of customer activity. The results of our study indicate potentials and challenges for an improvement of the shopping experience with AmI technologies. Based on our findings we discuss challenges and future developments for applying AmI technologies to shopping environments.},
  author    = {Reitberger, Wolfgang and Obermair, Christoph and Ploderer, Bernd and Meschtscherjakov, Alexander and Tscheligi, Manfred},
  booktitle = {Ambient Intelligence},
  doi       = {10.1007/978-3-540-76652-0_19},
  url       = {https://link.springer.com/chapter/10.1007/978-3-540-76652-0_19},
  isbn      = {978-3-540-76652-0},
  pages     = {314--331},
  series    = {Ambient Intelligence},
  publisher = {Springer},
  title     = {Enhancing the Shopping Experience with Ambient Displays: A Field Study in a Retail Store},
  year      = {2007},
  keywords  = {definition:no_definition, technology:display, data:activity, methodology:field_study, visualization:standard}
}

@inproceedings{white2009a,
  abstract  = {Menus play an important role in both information presentation and system control. We explore the design space of shake menus, which are intended for use in tangible augmented reality. Shake menus are radial menus displayed centered on a physical object and activated by shaking that object. One important aspect of their design space is the coordinate system used to present menu options. We conducted a within-subjects user study to compare the speed and efficacy of several alternative methods for presenting shake menus in augmented reality (world-referenced, display-referenced, and object-referenced), along with a baseline technique (a linear menu on a clipboard). Our findings suggest tradeoffs amongst speed, efficacy, and flexibility of interaction, and point towards the possible advantages of hybrid approaches that compose together transformations in different coordinate systems. We close by describing qualitative feedback from use and present several illustrative applications of the technique.},
  author    = {White, Sean and Feng, David and Feiner, Steven},
  booktitle = {8th IEEE International Symposium on Mixed and Augmented Reality},
  doi       = {10.1109/ISMAR.2009.5336500},
  url       = {https://ieeexplore.ieee.org/document/5336500},
  pages     = {39-48},
  series    = {ISMAR},
  title     = {Interaction and Presentation Techniques for Shake Menus in Tangible Augmented Reality},
  year      = {2009},
  keywords  = {definition:white_feiner, technology:AR, data:unclear, methodology:lab_study, visualization:N/A}
}

@inproceedings{white2009b,
  author    = {White, Sean and Feiner, Steven},
  title     = {SiteLens: Situated Visualization Techniques for Urban Site Visits},
  year      = {2009},
  isbn      = {9781605582467},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/1518701.1518871},
  doi       = {10.1145/1518701.1518871},
  abstract  = {Urban designers and urban planners often conduct site visits prior to a design activity to search for patterns or better understand existing conditions. We introduce SiteLens, an experimental system and set of techniques for supporting site visits by visualizing relevant virtual data directly in the context of the physical site, which we call situated visualization. We address alternative visualization representations and techniques for data collection, curation, discovery, comparison, manipulation, and provenance. A real use scenario is presented and two iterations of evaluation with faculty and students from the Columbia University Graduate School of Architecture, Planning and Preservation provide directions and insight for further investigation.},
  booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
  pages     = {1117–1120},
  numpages  = {4},
  location  = {Boston, MA, USA},
  series    = {CHI},
  keywords  = {definition:own_definition, technology:AR, data:environmental, methodology:field_study, visualization:custom}
}

@inbook{kalkofen2011,
  abstract  = {
  Visualizations in real world environments benefit from the visual interaction between real and virtual imagery. However, compared to traditional visualizations, a number of problems have to be solved in order to achieve effective visualizations within Augmented Reality (AR). This chapter provides an overview of techniques to handle the main obstacles in AR visualizations. It discusses spatial integration of virtual objects within real world environments, techniques to rearrange objects within mixed environments, and visualizations which adapt to its environmental context.},
  author    = {Kalkofen, Denis and Sandor, Christian and White, Sean and Schmalstieg, Dieter},
  booktitle = {Handbook of Augmented Reality},
  doi       = {10.1007/978-1-4614-0064-6_3},
  pages     = {65--98},
  publisher = {Springer},
  title     = {Visualization Techniques for Augmented Reality},
  url       = {https://arbook.icg.tugraz.at/schmalstieg/Schmalstieg_217.pdf},
  year      = {2011},
  keywords  = {definition:white_feiner, technology:AR, data:engineering, data:other, methodology:conceptual_work, visualization:standard, visualization:N/A}
}

@inproceedings{walsh2011,
  author    = {Walsh, James A. and Thomas, Bruce H.},
  title     = {Visualising Environmental Corrosion in Outdoor Augmented Reality},
  year      = {2011},
  isbn      = {9781920682972},
  publisher = {Australian Computer Society, Inc.},
  address   = {AUS},
  abstract  = {This paper provides a description of outdoor visualisation of environmental corrosion data. This system was developed to aid in the visual understanding of data from wireless sensors used to monitor large structures. Due to the laborious manual inspections required for large structures (such as bridges), wireless environmental sensors have been designed to automate this process. Our system visualizes this information in its real-world context using the Tinmith mobile outdoor augmented reality system. We provide an overview of the visualizations, outlining a user study that was conducted to determine the effectiveness of the visualizations in providing the user with context-sensitive information, along with the preliminary results of this study. The paper concludes with an overview of future work on the system and final thoughts.},
  booktitle = {Proceedings of the Twelfth Australasian User Interface Conference - Volume 117},
  pages     = {39–46},
  numpages  = {8},
  location  = {Perth, Australia},
  series    = {AUIC},
  doi       = {10.5555/2460616.2460621},
  url       = {https://crpit.scem.westernsydney.edu.au/confpapers/CRPITV117Walsh.pdf},
  keywords  = {definition:white_feiner, technology:AR, data:engineering, methodology:lab_study, visualization:custom, visualization:standard}
}



@article{vandemoere2012,
  author    = {Vande Moere, Andrew and Hill, Dan},
  title     = {Designing for the Situated and Public Visualization of Urban Data},
  journal   = {Journal of Urban Technology},
  volume    = {19},
  number    = {2},
  pages     = {25-46},
  year      = {2012},
  publisher = {Routledge},
  doi       = {10.1080/10630732.2012.698065},
  url       = {https://doi.org/10.1080/10630732.2012.698065},
  abstract  = { This paper investigates the concept of urban visualization, the visual representation of an urban environment through its intrinsic or related data, where its display is also situated within that physical environment. It describes how the principles behind public and urban displays can be combined with those of social visualization and persuasive computing in order to create discursive as well as pictorial representations that provide a better and potentially actionable understanding of urban issues to its inhabitants. We introduce the role of several related research fields, and analyze a set of representative case studies, taken from current best practice, academic research studies, and an experimental design studio course to highlight the typical issues involved in conceptualizing and implementing an urban visualization. Lastly, the paper proposes a set of design constraints that typically characterize an urban visualization, in order to guide the future design and evaluation of useful applications within the field. },
  keywords  = {definition:own_definition, technology:other, technology:analog, data:civic_data, methodology:field_study, visualization:standard}
}



@article{jose2014,
  author     = {José, Rui and Otero, Nuno and Cardoso, Jorge C. S.},
  title      = {Dimensions of Situatedness for Digital Public Displays},
  year       = {2015},
  issue_date = {January 2014},
  publisher  = {Hindawi Limited},
  address    = {London, GBR},
  volume     = {2014},
  issn       = {1687-5893},
  url        = {https://doi.org/10.1155/2014/474652},
  doi        = {10.1155/2014/474652},
  abstract   = {Public displays are often strongly situated signs deeply embedded in their physical, social, and cultural setting. Understanding how the display is coupled with on-going situations, its level of situatedness, provides a key element for the interpretation of the displays themselves but is also an element for the interpretation of place, its situated practices, and its social context. Most digital displays, however, do not achieve the same sense of situatedness that seems so natural in their nondigital counterparts. This paper investigates people's perception of situatedness when considering the connection between public displays and their context. We have collected over 300 photos of displays and conducted a set of analysis tasks involving focus groups and structured interviews with 15 participants. The contribution is a consolidated list of situatedness dimensions that should provide a valuable resource for reasoning about situatedness in digital displays and informing the design and development of display systems.},
  journal    = {Adv. in Hum.-Comp. Int.},
  month      = jan,
  articleno  = {16},
  numpages   = {1},
  keywords   = {definition:no_definition, technology:analog, technology:display, data:unclear, methodology:conceptual_work, visualization:N/A}
}


@inproceedings{claes2015,
  author    = {Claes, Sandy and Vande Moere, Andrew},
  title     = {The Role of Tangible Interaction in Exploring Information on Public Visualization Displays},
  year      = {2015},
  isbn      = {9781450336086},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2757710.2757733},
  doi       = {10.1145/2757710.2757733},
  abstract  = {A rising number of public displays are becoming equipped with tangible interfaces. Especially in the context of the visualization of data in the public realm, offering tangible interaction modalities might actively attract and engage passer-bys, and lead to increased information discovery.. We therefore present a novel public visualization installation that deploys different forms of tangible interaction in combination with a public display in order to communicate civic data to a lay audience. During a comparative, deployment-based study in an urban context, we compared three distinct tangible interaction modalities in terms of the types of engagement and insight generation they facilitated. We report on our findings and discuss a number of design recommendations for tangible interaction on public information displays.},
  booktitle = {Proceedings of the 4th International Symposium on Pervasive Displays},
  pages     = {201–207},
  numpages  = {7},
  keywords  = {},
  location  = {Saarbruecken, Germany},
  series    = {PerDis},
  keywords  = {definition:no_definition, technology:display, data:civic_data, methodology:field_study, methodology:lab_study, visualization:standard}
}

@article{valkanova2015,
  title    = {Public visualization displays of citizen data: Design, impact and implications},
  journal  = {International Journal of Human-Computer Studies},
  volume   = {81},
  pages    = {4-16},
  year     = {2015},
  note     = {Transdisciplinary Approaches to Urban Computing},
  issn     = {1071-5819},
  doi      = {https://doi.org/10.1016/j.ijhcs.2015.02.005},
  url      = {https://www.sciencedirect.com/science/article/pii/S1071581915000282},
  author   = {Valkanova, Nina and Jorda, Sergi and Vande Moere, Andrew},
  keywords = {},
  series   = {IJHCS},
  abstract = {In this paper we propose citizen-driven, public data visualization as a tool to support social and civic purposes in public spaces. We argue for the potential of this approach, motivating it with recent trends and developments in the areas of information visualization, urban computing, and urban screens, and we layout a transdisciplinary research approach and methodology. Through three studies approaching our research goal from design, empirical, and reflective perspectives, we show how visualization interfaces, situated in public spaces can improve perception, and lead to sustained behavior change; can increase social awareness and discourse; and can influence meaningful participation and a range of social interactions related to locally relevant topics. We conclude by discussing implications for the design, use and evaluation of citizen-driven public visualization as a tool increase public awareness, participation and discourse.},
  keywords = {definition:no_definition, technology:other, technology:mobile_device, technology:display, data:environmental, data:activity, methodology:field_study, visualization:artistic}
}

@inproceedings{thompson2016,
  abstract     = {In this paper, we present the results of a multi-year participatory design process exploring the space of educational AR experiences for STEM education targeted at students of various ages and abilities. Our participants included teachers, students (ages five to fourteen), educational technology experts, game designers, and HCI researchers. The work was informed by state educational curriculum guidelines. The activities included developing a set of design dimensions which guided our ideation process, iteratively designing, building, and evaluating six prototypes with our stakeholders, and collecting our observations regarding the use of AR STEM applications by target students.},
  author       = {Thompson, Ben and Levy, Laura and Lambeth, Amelia and Byrd, David and Alcaidinho, Joelle and Radu, Iulian and Gandy, Maribeth},
  booktitle    = {2016 IEEE International Symposium on Mixed and Augmented Reality (Adjunct)},
  organization = {IEEE},
  pages        = {53--58},
  title        = {Participatory design of STEM education AR experiences for heterogeneous student groups: exploring dimensions of tangibility, simulation, and interaction},
  year         = {2016},
  series       = {ISMAR-Adjunct},
  doi          = {10.1109/ISMAR-Adjunct.2016.0038},
  url          = {https://ieeexplore.ieee.org/document/7836459},
  keywords     = {definition:no_definition, technology:AR, data:science, methodology:co_design, visualization:N/A}
}

@inproceedings{zollman2016,
  author    = {Zollmann, Stefanie and Poglitsch, Christian and Ventura, Jonathan},
  booktitle = {International Conference on Image and Vision Computing New Zealand},
  title     = {VISGIS: Dynamic situated visualization for geographic information systems},
  year      = {2016},
  volume    = {},
  number    = {},
  pages     = {1-6},
  series          = {IVCNZ},
  url       = {https://ieeexplore.ieee.org/document/7804440},
  doi       = {10.1109/IVCNZ.2016.7804440},
  abstract  = {Situated Visualization techniques are visualization techniques that provide a presentation of information within its spatial context. Situated Visualization techniques have several advantages compared to traditional visualization techniques with the biggest advantage being providing the spatial relationship between data and the actual environment. However, Situated Visualization techniques are also subject to several challenges. In particular, Situated Visualization of data from geographic information systems (GIS) is exposed to a set of problems, such as limited visibility, legibility, information clutter and the limited understanding of spatial relationships. In this paper, we address the challenges of visibility, information clutter and understanding of spatial relationships with a set of dynamic Situated Visualization techniques that address the special needs of Situated Visualization of GIS data in particular for “street-view”-like perspectives as used for many navigation applications. The proposed techniques include dynamic annotation placement, dynamic label alignment and occlusion culling. We applied those techniques for two types of Situated Visualizations: Augmented Reality visualization and Indirect Augmented Reality using 360 Degree footage.},
  keywords  = {definition:white_feiner, technology:AR, technology:mobile_device, data:other, methodology:artefact/system, visualization:N/A}
}

@article{elsayed2016,
  title    = {Situated Analytics: Demonstrating immersive analytical tools with Augmented Reality},
  journal  = {Journal of Visual Languages & Computing},
  volume   = {36},
  pages    = {13-23},
  year     = {2016},
  issn     = {1045-926X},
  doi      = {https://doi.org/10.1016/j.jvlc.2016.07.006},
  url      = {https://www.sciencedirect.com/science/article/pii/S1045926X16300404},
  author   = {Neven A.M. ElSayed and Bruce H. Thomas and Kim Marriott and Julia Piantadosi and Ross T. Smith},
  keywords = {},
  abstract = {This paper introduces the use of Augmented Reality as an immersive analytical tool in the physical world. We present Situated Analytics, a novel combination of real-time interaction and visualization techniques that allows exploration and analysis of information about objects in the user's physical environment. Situated Analytics presents both situated and abstract summary and contextual information to a user. We conducted a user study to evaluate its use in three shopping analytics tasks, comparing the use of a Situated Analytics prototype with manual analysis. The results showed that users preferred the Situated Analytics prototype over the manual method, and that tasks were performed more quickly and accurately using the prototype.},
  keywords = {definition:white_feiner, technology:AR, data:other, methodology:lab_study, visualization:standard}
}

@inproceedings{engelke2016,
  author    = {Engelke, Ulrich and Hutson, Holly and Nguyen, Huyen and de Souza, Paulo},
  title     = {MelissAR: Towards Augmented Visual Analytics of Honey Bee Behaviour},
  year      = {2016},
  isbn      = {9781450340823},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2851581.2892367},
  doi       = {10.1145/2851581.2892367},
  abstract  = {We present the design and current prototype implementation of MelissAR, an augmented reality system for visual analytics of honey bee behaviour in the field. The system is intended to support bee keepers and other relevant users to monitor honey bee populations and to make effective decisions based on their status. The implementation of MelissAR is based on informed design choices with regard to usability in the field, effective communication of relevant information, and robustness to varying outdoor conditions.},
  booktitle = {Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems},
  pages     = {2057–2063},
  numpages  = {7},
  keywords  = {},
  location  = {San Jose, California, USA},
  series    = {CHI EA},
  keywords  = {definition:white_feiner, technology:AR, data:science, methodology:artefact/system, visualization:standard, visualization:custom}
}

@inproceedings{singhal2017,
  author    = {Singhal, Samarth and Odom, William and Bartram, Lyn and Neustaedter, Carman},
  title     = {Time-Turner: Data Engagement Through Everyday Objects in the Home},
  year      = {2017},
  isbn      = {9781450349918},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3064857.3079122},
  doi       = {10.1145/3064857.3079122},
  abstract  = {Families enjoy capturing digital media about their life and replaying moments, yet it is not always easy to do so. To explore this design space, we created a physical, ambient, and situated visualization prototype called Time-Turner specifically designed for a home setting that records video of family activities and allows families to review their past activities. We report our design requirements, design rationale and the implications of our work for future design researchers.},
  booktitle = {Proceedings of the 2017 ACM Conference Companion Publication on Designing Interactive Systems},
  pages     = {72–78},
  numpages  = {7},
  keywords  = {},
  location  = {Edinburgh, United Kingdom},
  series    = {DIS Companion},
  keywords  = {definition:willett, technology:physicalization, data:activity, data:personal, methodology:artefact/system, visualization:standard, visualization:artistic, visualization:physical}
}

@inproceedings{li2017,
  author    = {Li, Nico and Sharlin, Ehud and Sousa, Mario Costa},
  title     = {Duopography: Using Back-of-Device Multi-Touch Input to Manipulate Spatial Data on Mobile Tangible Interactive Topography},
  year      = {2017},
  isbn      = {9781450354103},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3132787.3139197},
  doi       = {10.1145/3132787.3139197},
  abstract  = {In this short paper we present the design of Duopography1, a dual-surface mobile tangible interface for spatial representation and manipulation of topography. The 3D physical topographic front of Duopography acts as a tangible interface, enabling sketching directly on the 3D terrain, as well as visual augmentation of the topography. At the same time, Duopography's flat back-of-device supports gestures that are hard to perform on the irregular front, allowing common interaction techniques such as panning and pinching. We contribute a prototype and the results of a preliminary evaluation of a dual-surface topography interface combining 3D printed front and a flat back-of-device.},
  booktitle = {SIGGRAPH Asia 2017 Mobile Graphics &amp; Interactive Applications},
  articleno = {20},
  numpages  = {6},
  keywords  = {},
  location  = {Bangkok, Thailand},
  series    = {SA},
  keywords = {definition:no_definition, technology:AR, technology:physicalization, data:consumer, data:other, methodology:lab_study, visualization:standard, visualization:physical}
}

@article{willett2017,
  author   = {Willett, Wesley and Jansen, Yvonne and Dragicevic, Pierre},
  doi      = {10.1109/TVCG.2016.2598608},
  issn     = {1077-2626},
  journal  = {IEEE TVCG},
  month    = jan,
  number   = {1},
  numpages = {10},
  pages    = {461--470},
  title    = {{Embedded Data Representations}},
  url      = {https://doi.org/10.1109/TVCG.2016.2598608},
  volume   = {23},
  year     = {2017},
  series          = {TVCG},
  abstract = {We introduce embedded data representations, the use of visual and physical representations of data that are deeply integrated with the physical spaces, objects, and entities to which the data refers. Technologies like lightweight wireless displays, mixed reality hardware, and autonomous vehicles are making it increasingly easier to display data in-context. While researchers and artists have already begun to create embedded data representations, the benefits, trade-offs, and even the language necessary to describe and compare these approaches remain unexplored. In this paper, we formalize the notion of physical data referents - the real-world entities and spaces to which data corresponds - and examine the relationship between referents and the visual and physical representations of their data. We differentiate situated representations, which display data in proximity to data referents, and embedded representations, which display data so that it spatially coincides with data referents. Drawing on examples from visualization, ubiquitous computing, and art, we explore the role of spatial indirection, scale, and interaction for embedded representations. We also examine the tradeoffs between non-situated, situated, and embedded data displays, including both visualizations and physicalizations. Based on our observations, we identify a variety of design challenges for embedded data representation, and suggest opportunities for future research and applications.},
  keywords = {definition:white_feiner, definition:own_definition, technology:AR, technology:VR, technology:mobile_device, technology:display, technology:physicalization, technology:analog, data:environmental, data:science, data:engineering, data:activity, data:personal, data:consumer, data:civic_data, data:other, methodology:conceptual_work, visualization:standard, visualization:custom, visualization:artistic, visualization:physical}
}

@inproceedings{hull2017,
  author    = {Hull, Carmen and Willett, Wesley},
  title     = {Building with Data: Architectural Models as Inspiration for Data Physicalization},
  year      = {2017},
  isbn      = {9781450346559},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3025453.3025850},
  doi       = {10.1145/3025453.3025850},
  abstract  = {In this paper we analyze the role of physical scale models in the architectural design process and apply insights from architecture for the creation and use of data physicalizations. Based on a survey of the architecture literature on model making and ten interviews with practicing architects, we describe the role of physical models as a tool for exploration and communication. From these observations, we identify trends in the use of physical models in architecture, which have the potential to inform the design of data physicalizations. We identify four functions of architectural modeling that can be directly adapted for use in the process of building rich data models. Finally, we discuss how the visualization community can apply observations from architecture to the design of new data physicalizations.},
  booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
  pages     = {1217–1264},
  numpages  = {48},
  keywords  = {},
  location  = {Denver, Colorado, USA},
  series    = {CHI},
  keywords = {definition:white_feiner, definition:willett, technology:physicalization, data:engineering, methodology:interviews, visualization:physical}
}

@inproceedings{bueschel2018,
  author    = {Büschel, Wolfgang and Mitschick, Annett and Dachselt, Raimund},
  title     = {Here and Now: Reality-Based Information Retrieval: Perspective Paper},
  year      = {2018},
  isbn      = {9781450349253},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3176349.3176384},
  doi       = {10.1145/3176349.3176384},
  abstract  = {Today, the widespread use of mobile devices allows users to search information "on the go", whenever and wherever they want, no longer confining Information Retrieval to classic desktop interfaces. We believe that technical advances in Augmented Reality will allow Information Retrieval to go even further, making use of both the users' surroundings and their abilities to interact with the physical world. In this paper, we present the fundamental concept of Reality-Based Information Retrieval, which combines the classic Information Retrieval process with Augmented Reality technologies to provide context-dependent search cues and situated visualizations of the query and the results. With information needs often stemming from real-world experiences, this novel combination has the potential to better support both Just-in-time Information Retrieval and serendipity. Based on extensive literature research, we propose a
  framework for Reality-Based Information Retrieval. We illustrate and discuss this framework and present two prototypical implementations, which we tested in small user studies. They demonstrate the feasibility of our concepts and inspired our discussion of notable challenges for further research in this novel and promising area.},
  booktitle = {Proceedings of the 2018 Conference on Human Information Interaction &amp; Retrieval},
  pages     = {171–180},
  numpages  = {10},
  keywords  = {},
  location  = {New Brunswick, NJ, USA},
  series    = {CHIIR},
  keywords = {definition:white_feiner, technology:AR, data:consumer, methodology:lab_study, visualization:N/A}
}

@article{thomas2018,
  title       = {{Situated Analytics}},
  author      = {Thomas, Bruce and Welch, Gregory and Dragicevic, Pierre and Elmqvist, Niklas and Irani, Pourang and Jansen, Yvonne and Schmalstieg, Dieter and Tabard, Aur{\'e}lien and ElSayed, Neven and Smith, Ross and Willett, Wesley},
  url         = {https://hal.inria.fr/hal-01947243},
  booktitle   = {{Immersive Analytics}},
  publisher   = {{Springer}},
  series      = {LNCS},
  volume      = {11190},
  pages       = {185-220},
  year        = {2018},
  month       = Oct,
  doi         = {10.1007/978-3-030-01388-2\_7},
  keywords    = {},
  pdf         = {https://hal.inria.fr/hal-01947243/file/SituatedAnalytics.pdf},
  hal_id      = {hal-01947243},
  hal_version = {v1},
  keywords    = {definition:willett, technology:AR, data:other, data:environmental, data:engineering, data:consumer, methodology:lab_study, visualization:standard}
}


@inproceedings{bressa2019,
  author    = {Bressa, Nathalie and Wannamaker, Kendra and Korsgaard, Henrik and Willett, Wesley and Vermeulen, Jo},
  title     = {Sketching and Ideation Activities for Situated Visualization Design},
  year      = {2019},
  isbn      = {9781450358507},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3322276.3322326},
  doi       = {10.1145/3322276.3322326},
  abstract  = {We report on findings from seven design workshops that used ideation and sketching activities to prototype new situated visualizations - representations of data that are displayed in proximity to the physical referents (such as people, objects, and locations) to which the data is related. Designing situated visualizations requires a fine-grained understanding of the context in which the visualizations are placed, as well as an exploration of different options for placement and form factors, which existing methods for visualization design do not account for. Focusing on small displays as a target platform, we reflect on our experiences of using a diverse range of sketching activities, materials, and prompts. Based on these observations, we identify challenges and opportunities for sketching and ideating situated visualizations. We also outline the space of design activities for situated visualization and highlight promising methods for both designers and researchers.},
  booktitle = {Proceedings of the 2019 on Designing Interactive Systems Conference},
  pages     = {173–185},
  numpages  = {13},
  keywords  = {},
  location  = {San Diego, CA, USA},
  series    = {DIS},
  keywords = {definition:white_feiner, definition:willett, technology:analog, data:other, data:environmental, data:consumer, data:personal, methodology:workshops, visualization:standard}
}

@inproceedings{demacedomorais2019,
  author    = {de Macêdo Morais, Luiz Augusto and Andrade, Nazareno and Costa de Sousa, Dandara Maria and Ponciano, Lesandro},
  booktitle = {2019 IEEE Pacific Visualization Symposium (PacificVis)},
  title     = {Defamiliarization, Representation Granularity, and User Experience: A Qualitative Study with Two Situated Visualizations},
  year      = {2019},
  volume    = {},
  number    = {},
  pages     = {92-101},
  doi       = {10.1109/PacificVis.2019.00019},
  url       = {https://ieeexplore.ieee.org/document/8781563},
  abstract  = {This work explores the user experience with two situated visualizations that lie on different points of design space. The first visualization - the Activity Clock - displays the aggregate presence of laboratory members into a wall clock. The second - Personal Activities - represents the same persons individually, in a conventional poster media. We interviewed 17 participants and leverage a theoretical lens of Continuous Engagement and Sense-Making to study how design decisions impact the user experience with respect to (1) which design factors attract users, (2) how design features affect users' understanding of the visualization, and (3) what kind of reflections are evoked by design. We discuss how the defamiliarizing effect of the Activity Clock plays a dual role in attracting users while also hindering their understanding of the data. We also consider the evidence that fine representation granularity in the Personal Activities evokes deeper reflections.},
  keywords = {definition:willett, technology:analog, data:activity, methodology:field_study, visualization:custom}
}

@inproceedings{coenen2019,
  author    = {Coenen, Jorgos and Houben, Maarten and Vande Moere, Andrew},
  title     = {Citizen Dialogue Kit: Public Polling and Data Visualization Displays for Bottom-Up Citizen Participation},
  year      = {2019},
  isbn      = {9781450362702},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3301019.3325160},
  doi       = {10.1145/3301019.3325160},
  abstract  = {In this demo we introduce our ongoing research on how to leverage the situated visualization of open and citizen science data within public space to inform and engage citizens. We developed an open-source toolkit, coined "Citizen Dialogue Kit" that is able to convey data visualizations on a set of interactive, wirelessly networked displays that can be freely positioned in urban space. The toolkit consists of a participative methodology to guide stakeholders with the choice of data and the design of its visualization, a set of off-the-shelf hardware components, and custom-made open source software that controls the whole system. We summarize the design of the toolkit and its initial deployment and conclude by discussing implications for urban visualization and future work.},
  booktitle = {Companion Publication of the 2019 on Designing Interactive Systems Conference 2019 Companion},
  pages     = {9–12},
  numpages  = {4},
  keywords  = {},
  location  = {San Diego, CA, USA},
  series    = {DIS Companion},
  keywords  = {definition:willett, definition:vandemoere_hill, technology:display, data:civic_data, methodology:field_study, visualization:standard, visualization:custom}
}



@inproceedings{marques2019,
  author    = {Marques, Bernardo and Santos, Beatriz Sousa and Araújo, Tiago and Martins, Nuno Cid and Alves, João Bernardo and Dias, Paulo},
  booktitle = {23rd International Conference Information Visualisation},
  pages     = {13-18},
  publisher = {IEEE},
  title     = {{Situated Visualization in The Decision Process Through Augmented Reality}},
  year      = {2019},
  doi       = {10.1109/IV.2019.00012},
  keywords  = {},
  series    = {IV},
  url       = {https://ieeexplore.ieee.org/document/8811981},
  abstract  = {The decision-making process and the development of decision support systems (DSS) have been enhanced by a variety of methods originated from information science, cognitive psychology and artificial intelligence over the past years. Situated visualization (SV) is a method to present data representations in context. Its main characteristic is to display data representations near the data referent. As augmented reality (AR) is becoming more mature, affordable and widespread, using it as a tool for SV becomes feasible in several situations. In addition, it may provide a positive contribution to more effective and efficient decision-making, as the users have contextual, relevant and appropriate information to endorse their choices. As new challenges and opportunities arise, it is important to understand the relevance of intertwining these fields. Based on a literature analysis, this paper addresses and discusses current areas of application, benefits, challenges and opportunities of using SV through AR to visualize data in context and to support a decision-making process and its importance in future DSS.},
  keywords  = {definition:white_feiner, definition:willett, technology:AR, data:civic_data, data:engineering, methodology:conceptual_work, visualization:standard}
}

@inproceedings{caggianese2019,
  author    = {Caggianese, Giuseppe and Colonnese, Valerio and Gallo, Luigi},
  booktitle = {2019 15th International Conference on Signal-Image Technology Internet-Based Systems (SITIS)},
  doi       = {10.1109/SITIS.2019.00069},
  pages     = {390-395},
  title     = {{Situated Visualization in Augmented Reality: Exploring Information Seeking Strategies}},
  year      = {2019},
  url       = {https://ieeexplore.ieee.org/document/9067881},
  abstract  = {In recent years augmented reality applications have been increasingly demonstrating the requirement for an interaction with information related to and directly shown in the surrounding environment. Situated information is visualized in its semantic and spatial context, building up an environment enhanced by an information level that dynamically adapts to the production of the information and to the actions of the user. The exploration and manipulation of this type of data through see-through augmented reality devices still represents a challenging task. The development of specific interaction strategies capable to mitigating the current limitations of augmented reality devices is essential. In this context, our contribution has been to design possible solutions to address some of these challenges allowing a dynamic interaction with situated information. Following the visual "information-seeking mantra" proposed by Shneiderman and introducing some "superpowers" for the users, in this work we present different strategies aimed at obtaining an overview and filtering, and acquiring details of a collection of situated data.},
  keywords  = {definition:white_feiner, definition:willett, definition:tatzgern, technology:AR, data:unclear, methodology:lab_study, visualization:N/A}
}

@article{francia2020,
  author   = {Matteo Francia and Matteo Golfarelli and Stefano Rizzi},
  doi      = {https://doi.org/10.1016/j.is.2020.101520},
  issn     = {0306-4379},
  journal  = {Information Systems},
  keywords = {},
  pages    = {101520},
  title    = {{A-BI+: A Framework for Augmented Business Intelligence}},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306437920300314},
  volume   = {92},
  year     = {2020},
  abstract = {Augmented reality allows users to superimpose digital information (typically, of operational type) upon real-world objects. The synergy of analytical frameworks and augmented reality opens the door to a new wave of situated analytics, in which users within a physical environment are provided with immersive analyses of local contextual data. In this paper, we propose an approach named A-BI+ (Augmented Business Intelligence) that, based on the sensed augmented context (provided by wearable and smart devices), proposes a set of relevant analytical queries to the user. This is done by relying on a mapping between the objects that can be recognized by the devices and the elements of the enterprise multidimensional cubes, and also by taking into account the queries preferred by users during previous interactions that occurred in similar contexts. A set of experimental tests evaluates the proposed approach in terms of efficiency, effectiveness, and user satisfaction.},
  keywords = {definition:thomas, technology:AR, data:other, methodology:lab_study, visualization:N/A}
}

@article{whitlock2020TVCG,
  author   = {Whitlock, Matt and Wu, Keke and Szafir, Danielle Albers},
  journal  = {IEEE Transactions on Visualization and Computer Graphics},
  title    = {Designing for Mobile and Immersive Visual Analytics in the Field},
  year     = {2020},
  volume   = {26},
  number   = {1},
  pages    = {503-513},
  series          = {TVCG},
  abstract = {Data collection and analysis in the field is critical for operations in domains such as environmental science and public safety. However, field workers currently face dataand platform-oriented issues in efficient data collection and analysis in the field, such as limited connectivity, screen space, and attentional resources. In this paper, we explore how visual analytics tools might transform field practices by more deeply integrating data into these operations. We use a design probe coupling mobile, cloud, and immersive analytics components to guide interviews with ten experts from five domains to explore how visual analytics could support data collection and analysis needs in the field. The results identify shortcomings of current approaches and target scenarios and design considerations for future field analysis systems. We embody these findings in FieldView, an extensible, open-source prototype designed to support critical use cases for situated field analysis. Our findings suggest the potential for integrating mobile and immersive technologies to enhance data's utility for various field operations and new directions for visual analytics tools to transform fieldwork.},
  keywords = {},
  doi      = {10.1109/TVCG.2019.2934282},
  issn     = {1941-0506},
  month    = {Jan},
  keywords = {definition:willett, technology:AR, technology:mobile_device, data:science, methodology:design_probe, visualization:standard}
}

@article{quach2020,
  author    = {Quang Quach and Bernhard Jenny},
  title     = {Immersive visualization with bar graphics},
  journal   = {Cartography and Geographic Information Science},
  volume    = {47},
  number    = {6},
  pages     = {471-480},
  year      = {2020},
  publisher = {Taylor & Francis},
  doi       = {10.1080/15230406.2020.1771771},
  url       = {https://doi.org/10.1080/15230406.2020.1771771},
  abstract  = {Augmented reality (AR) enables the creation of immersive situated visualizations. These visualizations blend with the real world so that the user perceives the virtual objects as physically present. We propose two novel immersive situated visualizations for geospatial quantitative point data that link AR bar graphics in the real environment with a virtual AR chart or map. The Egocentric Bar Chart combines virtual bars placed in a real landscape with a virtual 360° bar chart arranged around the user. The Circular Map with Bars combines bars in a real landscape with a 360° virtual map showing bar graphics. These visualizations are designed for inspecting the location and attributes of individual points, as well as understanding the spatial and non-spatial patterns of a set of points. We conducted a study with 12 participants to evaluate reading accuracy and efficiency of the two visualizations, with a view from a skyscraper at the center of the Melbourne metropolitan area. To better control confounding factors, the study used a virtual environment instead of an AR setup. We found that the two suggested visualizations performed better than bars placed in a landscape, and received positive feedback from study participants. The Egocentric Bar Chart visualization was most accurate when estimating and comparing values, and we identified promising improvements for the Circular Map with Bars visualization. Our results demonstrate the potential of AR technology for the communication of geospatial point data with immersive situated bar graphics in an outdoor environment. },
  keywords  = {definition:white_feiner, definition:willett, technology:VR, data:unclear, methodology:lab_study, visualization:standard}
}


@inproceedings{chen2020,
  author    = {Chen, Zhutian and Tong, Wai and Wang, Qianwen and Bach, Benjamin and Qu, Huamin},
  title     = {Augmenting Static Visualizations with PapARVis Designer},
  year      = {2020},
  isbn      = {9781450367080},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3313831.3376436},
  doi       = {10.1145/3313831.3376436},
  abstract  = {This paper presents an authoring environment for augmenting static visualizations with virtual content in augmented reality.Augmenting static visualizations can leverage the best of both physical and digital worlds, but its creation currently involves different tools and devices, without any means to explicitly design and debug both static and virtual content simultaneously. To address these issues, we design an environment that seamlessly integrates all steps of a design and deployment workflow through its main features: i) an extension to Vega, ii) a preview, and iii) debug hints that facilitate valid combinations of static and augmented content. We inform our design through a design space with four ways to augment static visualizations. We demonstrate the expressiveness of our tool through examples, including books, posters, projections, wall-sized visualizations. A user study shows high user satisfaction of our environment and confirms that participants can create augmented visualizations in an average of 4.63 minutes.},
  booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
  pages     = {1–12},
  numpages  = {12},
  keywords  = {},
  location  = {Honolulu, HI, USA},
  series    = {CHI},
  keywords = {definition:willett, technology:AR, data:science, data:activity, data:consumer, data:personal, data:other, methodology:lab_study, visualization:standard, visualization:custom}
}

@article{lobo2020,
  title       = {{Opportunities and challenges for augmented reality situated geographical visualization}},
  author      = {Lobo, Mar{\'i}a-Jes{\'u}s and Christophe, Sidonie},
  url         = {https://hal.archives-ouvertes.fr/hal-02878352},
  journal     = {{ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences}},
  series      = {ISPRS},
  volume      = {V-4},
  pages       = {163 - 170},
  year        = {2020},
  month       = Aug,
  doi         = {10.5194/isprs-annals-V-4-2020-163-2020},
  keywords    = {},
  pdf         = {https://hal.archives-ouvertes.fr/hal-02878352/file/ISPRS_AR-6.pdf},
  hal_id      = {hal-02878352},
  hal_version = {v1},
  keywords = {definition:white_feiner, definition:willett, technology:AR, data:other, methodology:conceptual_work, visualization:N/A}
}

@inproceedings{prouzeau2020,
  author    = {Prouzeau, Arnaud and Wang, Yuchen and Ens, Barrett and Willett, Wesley and Dwyer, Tim},
  title     = {Corsican Twin: Authoring In Situ Augmented Reality Visualisations in Virtual Reality},
  year      = {2020},
  isbn      = {9781450375351},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3399715.3399743},
  doi       = {10.1145/3399715.3399743},
  abstract  = {We introduce Corsican Twin, a tool for authoring augmented reality data visualisations in virtual reality using digital twins. The system provides users with the contextual information necessary to design embedded and situated data visualisations in a safe and convenient remote setting. We created system via a co-design process which involved people with little or no programming experience. Using the system, we illustrate three potential use cases for situated visualizations in the context of building maintenance, including: (1) on-site equipment debugging and diagnosis; (2) remote incident playback; and (3) operations simulations for future buildings. From feedback gathered during formative evaluations of our prototype tool with domain experts, we discuss implications, opportunities, and challenges for future in situ visualisation design tools.},
  booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
  articleno = {11},
  numpages  = {9},
  keywords  = {},
  location  = {Salerno, Italy},
  series    = {AVI},
  keywords = {definition:willett, definition:thomas, technology:AR, technology:VR, data:engineering, methodology:co_design, visualization:standard}
}

@inproceedings{merino2020,
  author    = {Merino, Leonel and Sotomayor-G\'{o}mez, Boris and Yu, Xingyao and Salgado, Ronie and Bergel, Alexandre and Sedlmair, Michael and Weiskopf, Daniel},
  title     = {Toward Agile Situated Visualization: An Exploratory User Study},
  year      = {2020},
  isbn      = {9781450368193},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3334480.3383017},
  doi       = {10.1145/3334480.3383017},
  abstract  = {We introduce AVAR, a prototypical implementation of an agile situated visualization (SV) toolkit targeting liveness, integration, and expressiveness. We report on results of an exploratory study with AVAR and seven expert users. In it, participants wore a Microsoft HoloLens device and used a Bluetooth keyboard to program a visualization script for a given dataset. To support our analysis, we (i) video recorded sessions, (ii) tracked users' interactions, and (iii) collected data of participants' impressions. Our prototype confirms that agile SV is feasible. That is, liveness boosted participants' engagement when programming an SV, and so, the sessions were highly interactive and participants were willing to spend much time using our toolkit (i.e., median ≥ 1.5 hours). Participants used our integrated toolkit to deal with data transformations, visual mappings, and view transformations without leaving the immersive environment. Finally, participants benefited from our expressive toolkit and employed multiple of the available features when programming an SV.},
  booktitle = {Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems},
  pages     = {1–7},
  numpages  = {7},
  keywords  = {},
  location  = {Honolulu, HI, USA},
  series    = {CHI EA},
  keywords = {definition:thomas, technology:AR, data:unclear, methodology:lab_study, visualization:standard}
}

@article{perovich2020,
  author    = {Perovich, Laura J. and Wylie, Sara Ann and Bongiovanni,  Roseann},
  journal   = {IEEE Transactions on Visualization & Computer Graphics},
  title     = {Chemicals in the Creek: designing a situated data physicalization of open government data with the community},
  year      = {2021},
  volume    = {27},
  number    = {02},
  issn      = {1941-0506},
  pages     = {913-923},
  keywords  = {},
  doi       = {10.1109/TVCG.2020.3030472},
  publisher = {IEEE Computer Society},
  address   = {Los Alamitos, CA, USA},
  month     = {feb},
  series    = {TVCG},
  abstract  = {Over the last decade growing amounts of government data have been made available in an attempt to increase transparency and civic participation, but it is unclear if this data serves non-expert communities due to gaps in access and the technical knowledge needed to interpret this “open” data. We conducted a two-year design study focused on the creation of a community-based data display using the United States Environmental Protection Agency data on water permit violations by oil storage facilities on the Chelsea Creek in Massachusetts to explore whether situated data physicalization and Participatory Action Research could support meaningful engagement with open data. We selected this data as it is of interest to local groups and available online, yet remains largely invisible and inaccessible to the Chelsea community. The resulting installation, Chemicals in the Creek, responds to the call for community-engaged visualization processes and provides an application of situated methods of data representation. It proposes event-centered and power-aware modes of engagement using contextual and embodied data representations. The design of Chemicals in the Creek is grounded in interactive workshops and we analyze it through event observation, interviews, and community outcomes. We reflect on the role of community engaged research in the Information Visualization community relative to recent conversations on new approaches to design studies and evaluation.},
  keywords = {definition:willett, technology:physicalization, data:environmental, methodology:field_study, visualization:physical, visualization:artistic}
}


@article{weiss2020,
  author   = {Weiß, Maximilian and Angerbauer, Katrin and Voit, Alexandra and Schwarzl, Magdalena and Sedlmair, Michael and Mayer, Sven},
  journal  = {IEEE Transactions on Visualization and Computer Graphics},
  title    = {Revisited: Comparison of Empirical Methods to Evaluate Visualizations Supporting Crafting and Assembly Purposes},
  year     = {2021},
  volume   = {27},
  number   = {2},
  pages    = {1204-1213},
  series   = {TVCG},
  abstract = {Ubiquitous, situated, and physical visualizations create entirely new possibilities for tasks contextualized in the real world, such as doctors inserting needles. During the development of situated visualizations, evaluating visualizations is a core requirement. However, performing such evaluations is intrinsically hard as the real scenarios are safety-critical or expensive to test. To overcome these issues, researchers and practitioners adapt classical approaches from ubiquitous computing and use surrogate empirical methods such as Augmented Reality (AR), Virtual Reality (VR) prototypes, or merely online demonstrations. This approach's primary assumption is that meaningful insights can also be gained from different, usually cheaper and less cumbersome empirical methods. Nevertheless, recent efforts in the Human-Computer Interaction (HCI) community have found evidence against this assumption, which would impede the use of surrogate empirical methods. Currently, these insights rely on a single investigation of four interactive objects. The goal of this work is to investigate if these prior findings also hold for situated visualizations. Therefore, we first created a scenario where situated visualizations support users in do-it-yourself (DIY) tasks such as crafting and assembly. We then set up five empirical study methods to evaluate the four tasks using an online survey, as well as VR, AR, laboratory, and in-situ studies. Using this study design, we conducted a new study with 60 participants. Our results show that the situated visualizations we investigated in this study are not prone to the same dependency on the empirical method, as found in previous work. Our study provides the first evidence that analyzing situated visualizations through different empirical (surrogate) methods might lead to comparable results.},
  keywords = {},
  doi      = {10.1109/TVCG.2020.3030400},
  issn     = {1941-0506},
  month    = {Feb},
  keywords = {definition:schmalstieg_hoellerer, technology:AR, technology:VR, technology:other, data:other,  methodology:lab_study, visualization:standard, visualization:custom}
}

@article{morais2020,
  author    = {Morais, Luiz and Jansen, Yvonne and Andrade, Nazareno and Dragicevic, Pierre},
  journal   = {IEEE Transactions on Visualization & Computer Graphics},
  title     = {Showing Data about People: A Design Space of Anthropographics},
  year      = {5555},
  volume    = {},
  number    = {01},
  issn      = {1941-0506},
  pages     = {1-1},
  keywords  = {},
  doi       = {10.1109/TVCG.2020.3023013},
  publisher = {IEEE Computer Society},
  address   = {Los Alamitos, CA, USA},
  month     = {sep},
  series    = {TVCG},
  abstract  = {When showing data about people, visualization designers and data journalists often use design strategies that presumably help the audience relate to those people. The term anthropographics has been recently coined to refer to this practice and the resulting visualizations. Anthropographics is a rich and growing area, but the work so far has remained scattered. Despite preliminary empirical work and a few web essays written by practitioners, there is a lack of a clear language for thinking about and communicating about anthropographics. We address this gap by introducing a conceptual framework and a design space for anthropographics. Our design space consists of seven elementary design dimensions that can be reasonably hypothesized to have some effect on prosocial feelings or behavior. It extends a previous design space and is informed by an analysis of 105 visualizations collected from newspapers, websites, and research papers. We use our conceptual framework and design space to discuss trade-offs, common design strategies, as well as future opportunities for design and research in the area of anthropographics.},
  keywords = {definition:willett, technology:physicalization, technology:display, technology:other, technology:analog, data:other, data:personal, data:consumer, data:civic_data, methodology:conceptual_work, visualization:standard, visualization:custom, visualization:physical, visualization:artistic}
}

@inproceedings{guarese2020augmented,
  author    = {Guarese, Renan and Becker, Jo\~{a}o and Fensterseifer, Henrique and Walter, Marcelo and Freitas, Carla and Nedel, Luciana and Maciel, Anderson},
  title     = {Augmented Situated Visualization for Spatial and Context-Aware Decision-Making},
  year      = {2020},
  isbn      = {9781450375351},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3399715.3399838},
  doi       = {10.1145/3399715.3399838},
  abstract  = {Whenever accessing indoor spaces such as classrooms or auditoriums, people might attempt to analyze and choose an appropriate place to stay while attending an event. Several criteria may be accounted for, and most are not always self-evident or trivial. This work proposes the use of data visualization allied to an Augmented Reality (AR) user interface to help users defining the most convenient seats to take. We consider sets of arbitrary demands and project information directly atop the seats and all around the room. Users can also narrow down the search by switching and combining the attributes being displayed, e.g., temperature, wheelchair accessibility. The proposed approach was tested against a comparable 2D interactive visualization of the same data in usability assessments of seat-choosing tasks with a set of users (N = 16) to validate the solution. Qualitative and quantitative data indicated that the AR-based solution is promising, suggesting that AR may help users make more accurate decisions, even in an ordinary daily task. Regarding Augmented Situated Visualization, our results open new avenues for the exploration of context-aware data.},
  booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
  articleno = {48},
  numpages  = {5},
  keywords  = {},
  location  = {Salerno, Italy},
  series    = {AVI},
  keywords = {definition:thomas, technology:AR, data:environmental, data:engineering, methodology:lab_study, visualization:standard, visualization:custom}
}

@inproceedings{alallah2020,
  author    = {Alallah, Fouad and Sakamoto, Yumiko and Irani, Pourang},
  title     = {Exploring the Need and Design for Situated Video Analytics},
  year      = {2020},
  isbn      = {9781450379434},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3385959.3418458},
  doi       = {10.1145/3385959.3418458},
  abstract  = { Visual video analytics research, stemming from data captured by surveillance cameras, have mainly focused on traditional computing paradigms, despite emerging platforms including mobile devices. We investigate the potential for situated video analytics, which involves the inspection of video data in the actual environment where the video was captured [14]. Our ultimate goal is to explore the means to visually explore video data effectively, in situated contexts. We first investigate the performance of visual analytic tasks in situated vs. non-situated settings. We find that participants largely benefit from environmental cues for many analytic tasks. We then pose the question of how best to represent situated video data. To answer this, in a design session we explore end-users’ views on how to capture such data. Through the process of sketching, participants leveraged being situated, and explored how being in-situ influenced the participants’ integration of their designs. Based on these two elements, our paper proposes the need to develop novel spatial analytic user interfaces to support situated video analysis.},
  booktitle = {Symposium on Spatial User Interaction},
  articleno = {15},
  numpages  = {11},
  keywords  = {},
  location  = {Virtual Event, Canada},
  series    = {SUI},
  keywords = {definition:bressa_ducros, technology:AR, technology:mobile_device, data:other, methodology:lab_study, methodology:workshops, visualization:custom}
}

@inproceedings{whitlock2020ISMAR,
  author    = {Whitlock, Matt and Szafir, Danielle Albers and Gruchalla, Kenny},
  booktitle = {IEEE International Symposium on Mixed and Augmented Reality},
  title     = {HydrogenAR: Interactive Data-Driven Presentation of Dispenser Reliability},
  year      = {2020},
  volume    = {},
  number    = {},
  pages     = {704-712},
  series    = {ISMAR},
  abstract  = {When delivering presentations to a co-located audience, we typically use slides with text and 2D graphics to complement the spoken narrative. Though presentations have largely been explored on 2D media, augmented reality (AR) allows presentation designers to add data and augmentations to existing physical infrastructure on display. This coupling could provide a more engaging experience to the audience and support comprehension. With HydrogenAR, we present a novel application that leverages the benefits of data-driven storytelling with those of AR to explain the unique challenges of hydrogen dispenser reliability. Utilizing physical props, situated data, and virtual augmentations and animations, HydrogenAR serves as a unique presentation tool, particularly critical for stakeholders, tour groups, and VIPs. HydrogenAR is a product of multiple collaborative design iterations with a local Hydrogen Fuel research team and is evaluated through interviews with team members and a user study with end-users to evaluate the usability and quality of the interactive AR experience. Through this work, we provide design considerations for AR data-driven presentations and discuss how AR could be used for innovative content delivery beyond traditional slide-based presentations.},
  keywords  = {},
  doi       = {10.1109/ISMAR50242.2020.00101},
  issn      = {1554-7868},
  month     = {Nov},
  keywords = {definition:thomas, technology:AR, data:engineering, methodology:lab_study, visualization:standard}
}

@inproceedings{ens2021grandchallenges,
  author    = {Ens, Barrett and Bach, Benjamin and Cordeil, Maxime and Engelke, Ulrich and Serrano, Marcos and Willett, Wesley and Prouzeau, Arnaud and Anthes, Christoph and B\"{u}schel, Wolfgang and Dunne, Cody and Dwyer, Tim and Grubert, Jens and Haga, Jason H. and Kirshenbaum, Nurit and Kobayashi, Dylan and Lin, Tica and Olaosebikan, Monsurat and Pointecker, Fabian and Saffo, David and Saquib, Nazmus and Schmalstieg, Dieter and Szafir, Danielle Albers and Whitlock, Matt and Yang, Yalong},
  title     = {Grand Challenges in Immersive Analytics},
  year      = {2021},
  isbn      = {9781450380966},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3411764.3446866},
  abstract  = { Immersive Analytics is a quickly evolving field that unites several areas such as visualisation, immersive environments, and human-computer interaction to support human data analysis with emerging technologies. This research has thrived over the past years with multiple workshops, seminars, and a growing body of publications, spanning several conferences. Given the rapid advancement of interaction technologies and novel application domains, this paper aims toward a broader research agenda to enable widespread adoption. We present 17 key research challenges developed over multiple sessions by a diverse group of 24 international experts, initiated from a virtual scientific workshop at ACM CHI 2020. These challenges aim to coordinate future work by providing a systematic roadmap of current directions and impending hurdles to facilitate productive and effective applications for Immersive Analytics.},
  booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
  articleno = {459},
  series    = {CHI},
  numpages  = {17},
  keywords = {definition:willett, technology:AR, technology:VR, data:science, data:engineering, data:other, methodology:conceptual_work, visualization:standard, visualization:custom}
}

@article{guarese2021augmented,
  title     = {Augmented situated visualization methods towards electromagnetic compatibility testing},
  author    = {Guarese, Renan and Andreasson, Pererik and Nilsson, Emil and Maciel, Anderson},
  abstract  = {In electrical engineering, hardware experts often need to analyze electromagnetic radiation data to detect any external interference or anomaly. The field that studies this sort of assessment is called electromagnetic compatibility (EMC). As a way to support EMC analysis, we propose the use of Augmented Situated Visualization (ASV) to supply professionals with visual and interactive information that helps them to comprehend that data, however situating it where it is most relevant in its spatial context. Users are able to interact with the visualization by changing the attributes being displayed, comparing the overlaps of multiple fields, and extracting data, as a way to refine their search. The solutions being proposed in this work were tested against each other in comparable 2D and 3D interactive visualizations of the same data in a series of data-extraction assessments with users, as a means to validate the approaches. Results exposed a correctness-time trade-off between the interaction methods. The hand-based techniques (Hand Slider and Touch Lens) were the least error-prone, being near to half as error-inducing as the gaze-based method. Touch Lens also performed as the least time-consuming method, taking in average less than half of the average time required by the others. For the visualization methods tested, the 2D ray casts presented a higher usability score and lesser workload index than the 3D topology view, however exposing over two times the error ratio. Ultimately, this work exposes how AR can help users to have better performances in a decision-making context, particularly in EMC related tasks, while also furthering the research in the ASV field.}
  journal   = {Computers \& graphics},
  volume    = {94},
  pages     = {1--10},
  year      = {2021},
  series    = {Computers & Graphics},
  publisher = {Elsevier},
  keywords = {definition:willett, technology:AR, data:engineering, methodology:lab_study, visualization:custom}
}

@inproceedings{lee2021,
author = {Lee, Bokyung and Lee, Michael and Mogk, Jeremy and Goldstein, Rhys and Bibliowicz, Jacobo and Tessier, Alexander},
title = {{Designing a Multi-Agent Occupant Simulation System to Support Facility Planning and Analysis for COVID-19}},
year = {2021},
isbn = {9781450384766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461778.3462030},
doi = {10.1145/3461778.3462030},
booktitle = {Proc.  DIS '21},
pages = {15–30},
numpages = {16},
location = {Virtual Event, USA},
series = {DIS '21},
keywords = {definition:no_definition, technology:other, data:other, methodology:interviews, visualization:custom, visualization:standard}
}


@inproceedings{wannamaker2021,
author = {Wannamaker, Kendra A. and Kollannur, Sandeep Zechariah George and D\"{o}rk, Marian and Willett, Wesley},
title = {{I/O Bits: User-Driven, Situated, and Dedicated Self-Tracking}},
year = {2021},
isbn = {9781450384766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461778.3462138},
doi = {10.1145/3461778.3462138},
booktitle = {Proc.  DIS '21},
pages = {523–537},
numpages = {15},
location = {Virtual Event, USA},
series = {DIS '21},
keywords = {definition:willett, technology:display, data:personal, methodology:workshops, methodology:field_study, visualization:custom}
}

@inbook{lin2021,
author = {Lin, Tica and Singh, Rishi and Yang, Yalong and Nobre, Carolina and Beyer, Johanna and Smith, Maurice A. and Pfister, Hanspeter},
title = {{Towards an Understanding of Situated AR Visualization for Basketball Free-Throw Training}},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445649},
booktitle = {Proc.  CHI '21},
articleno = {461},
numpages = {13},
keywords = {definition:no_definition, technology:AR, technology:display, data:other, methodology:lab_study, visualization:custom}
}

@inproceedings{Satkowski2021,
author = {Satkowski, Marc and Dachselt, Raimund},
title = {{Investigating the Impact of Real-World Environments on the Perception of 2D Visualizations in Augmented Reality}},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445330},
booktitle = {Proc.  CHI '21},
articleno = {522},
numpages = {15},
keywords = {definition:willett, technology:AR, data:unclear, methodology:lab_study, visualization:custom}
}
